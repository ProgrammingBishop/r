---
title:  "Project 1 4042 sbishop3"
author: "Scott Bishop"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

- A report - 3 pages maximum, .pdf only, that provides the details of your code, e.g., pre-processing, some technical details or implementation details (if not trivial) of the models you use, etc.

- In addition, report the accuracy (see evaluation metric given below), running time of your code and the computer system you use (e.g., Macbook Pro, 2.53 GHz, 4GB memory, or AWS t2.large). You DO NOT need to submit the part of the code related to the evaluation you conduct.

- Build TWO prediction models. Always include a tree-based ensemble model, e.g., randomForest, and/or boosting tree. 

- After running your Rcode, we should see TWO txt files in the same directory named "mysubmission1.txt" and "mysubmission2.txt". Each submission file correspoonds to a prediction on the test data.

- For each of your two models, prediction error is calculated as the averaged RMSE over 3 splits. Full credit if one of the two errors is below 0.132 and extra credit if below 0.120

```{r, warning = FALSE, echo = FALSE, message = FALSE, include = FALSE}

# Set up Environment
# ==================================================

setwd('D:\\git\\rMachineLearning\\statisitcalLearning\\amesHousingAnalysis')
ames <- read.csv("./data/ames.csv")
set.seed(4042)

```

```{r, warning = FALSE, echo = FALSE, message = FALSE, include = FALSE}

# Install Packages
# ==================================================
packages <- c("DescTools", "tree", "glmnet", "randomForest", "xgboost", "gbm", "pls")   
init.pkg <- setdiff( packages, rownames( installed.packages() ) )  

if ( length(init.pkg) > 0 ) { install.packages(init.pkg) } 

lapply(packages, require, character.only = TRUE)

```



```{r}

# Find Majority Factor Function
# ==================================================

get.maj.factors <- function(vars, thresh)
{
  vars.to.remove <- character()
  denominator    <- nrow(split$train)
  
  # For Each Factor Variable
  for ( var in 1:length(vars) )
  {
    # And For Each Level of Current Factor Variable
    for ( level in 1:length( table(vars[[var]]) ) )
    {
      # Calculate it's Proportion of Rows
      numerator  <- table( vars[[var]] )[[level]]
      maj.factor <- numerator / denominator
      
      # If this Level has 95% Majority, Flag it For Removal
      if ( maj.factor >= thresh )
      {
        vars.to.remove[ length(vars.to.remove) + 1 ] <- names(factor.vars[var])
        level <- length( table(vars[[var]]) )
      }
    }
  }
  
  return(vars.to.remove)
}

```



```{r}

# Locate Columns with Missing Values
# ==================================================

# Replace Missing Values
set.missing.values <- function(the.data)
{
  # For Each Variable
  for( col in 1:ncol(the.data) )
  {
    # If Numeric Replace with Mean
    if ( is.numeric( the.data[, col ] ) )
    {
      the.data[ is.na( the.data[, col ] ), col ] <- round( mean( the.data[, col ], na.rm = TRUE), digits = 0 )
    }
    
    # If Categorical Replace with Most Common Category
    else if ( is.character( the.data[, col ] ) )
    {
      the.data[ is.na( the.data[, col ] ), col ] <- 
        names( table( the.data[, col ] )[ which.max( table( the.data[, col ] ) ) ] )
    }
  }
  
  return(the.data) 
}

```



```{r}

# Refactorize Factor Variables
# ==================================================

# Overall_QUal
refactor.overall_qual <- function(variable) 
{
  levels(variable)[levels(variable) %in% c("Very_Poor", "Poor", "Fair",  "Below_Average" ) ]  <- "low"
  levels(variable)[levels(variable) %in% c("Average",   "Above_Average", "Good" ) ]           <- "avg"
  levels(variable)[levels(variable) %in% c("Very_Good", "Excellent",     "Very_Excellent" ) ] <- "high"
  
  return(variable)
}


# Year Partition Function
get.year.partitions <- function(the.data, variable)
{
  # Build Tree
  tree.model <- tree(Sale_Price ~ variable, data = the.data, mindev = 0.0015)
  left.split <- tree.model$frame$splits[, 1]
  splits     <- left.split[left.split != ""]
  
  # Coerce Leaf Nodes into Integers
  for ( leaf in 1:length(splits) )
  {
    splits[leaf] <- gsub( "<", "", splits[leaf] )
  }
  
  splits <- as.numeric(splits)
}


# Set New Year Levels
set.year.levels <- function(nodes, variable)
{
  variable     <- as.numeric( variable )
  new.variable <- rep( 0, length(variable) )
  
  min.year <- floor( min(nodes) )
  max.year <- floor( max(nodes) )
  
  # For All Rows Above Min and Below Max
  for ( row in 1:length(variable) )
  {
    # Assign Lower and Upper Bound Nodes
    if ( variable[row] <= min.year ) { new.variable[row] <- paste("<", min.year, sep = "" ) }
    if ( variable[row] >= max.year ) { new.variable[row] <- paste(">", max.year, sep = "" ) }
    
    # For Each Possible Category
    for ( node in 2:( length(nodes) ) ) {
      # Check If Node Falls into Interval
      if ( variable[row] <= nodes[node] && new.variable[row] == 0 )
      {
        left              <- as.character( floor( nodes[node - 1] + 1 ) )
        right             <- as.character( floor( nodes[node    ] ) )
        new.variable[row] <- paste( left, " - ", right, sep = "" )
      }
    }  
  }
  
  return(new.variable)
}

```



```{r}

# Winsorization
# ==================================================

# Winsorize Numerical Values
winsorize.values <- function(data)
{
  train <- data
  
  # For Each Variable
  for( col in 2:(ncol(train) - 1) )
  {
    # Winsorize Numerical Columns
    if ( is.numeric( train[, col ] ) )
    {
      train[, col ] <- Winsorize( train[, col ], 
                                  minval = NULL, maxval = NULL, 
                                  probs = c(0.05, 0.95), na.rm = FALSE )
    }
  }
  
  return(train) 
}

```



```{r, warning = FALSE, echo = FALSE, message = FALSE}

# Partition Function
# ==================================================

make.split <- function(data)
{
    indexes <- sample( seq_len(nrow(data)), 
                       size = round(nrow(data) * 0.70) )

    train.csv <- data[ indexes, ]
    test.csv  <- data[-indexes, ]
    
    return( list("train"= train.csv,
                 "test"  = test.csv) )
}

split <- make.split(ames)

# Remove Longitude and Latitude
split$train <- split$train[, -c(81, 82)]
split$test  <- split$test [, -c(81, 82)]

```



```{r}

# Preprocess Partitions
# ==================================================

preprocess <- function(partition)
{
  # Drop Factor Variables with Majority Class
  factor.vars  <- partition[, sapply(partition, is.factor) ]
  drop.columns <- get.maj.factors(factor.vars, 0.95)
  partition    <- partition[, !( names(partition) %in% drop.columns ) ]
  
  # Fix NA Values
  na.columns <- colnames( partition )[ colSums( is.na( partition ) ) > 0 ]
  if ( length(na.columns) > 0 ) { partition <- set.missing.values(partition) }
  
  # Refactor Overall_Qual
  partition$Overall_Qual <- refactor.overall_qual( partition$Overall_Qual )
  
  # Winsorize Variables
  partition <- cbind( "PID" = partition[, 1], winsorize.values(partition[, -1]) )
  
  return( partition )
}

split$train <- preprocess(split$train)
split$test  <- preprocess(split$test)

```



```{r}

# Refactor Categories
# ==================================================
  
# New Year_Built Levels
year_built             <- sort( get.year.partitions(split$train, split$train$Year_Built) )
split$train$Year_Built <- factor( set.year.levels(year_built, split$train$Year_Built) )
split$test$Year_Built  <- factor( set.year.levels(year_built, split$test$Year_Built) )

# New Year_Remod_add
year_remod_add             <- sort( get.year.partitions(split$train, split$train$Year_Remod_Add) )
split$train$Year_Remod_Add <- factor( set.year.levels( year_remod_add, split$train$Year_Remod_Add) )
split$test$Year_Remod_Add  <- factor( set.year.levels( year_remod_add, split$test$Year_Remod_Add) )

# New Garage_Yr_blt
garage_yr_blt             <- sort( get.year.partitions(split$train, split$train$Garage_Yr_Blt) )
split$train$Garage_Yr_Blt <- factor( set.year.levels( garage_yr_blt, split$train$Garage_Yr_Blt) )
split$test$Garage_Yr_Blt  <- factor( set.year.levels( garage_yr_blt, split$test$Garage_Yr_Blt) )

```



```{r}

# Get Lambda Parameter Function
# ==================================================

get.lambda.lasso <- function(data, y.limit)
{
    lambda.matrix <- matrix( 0, nrow = 50, ncol = 4 )
    
    for ( i in 1:nrow( lambda.matrix ) )
    {
        model.cv <- cv.glmnet( as.matrix( data[, 2:( ncol(data) - 1) ] ), 
                               as.matrix( data[, ncol(data) ] ) )
        
        lambda.matrix[i, 1] <- min(model.cv$lambda)
        lambda.matrix[i, 2] <- max(model.cv$lambda)
        lambda.matrix[i, 3] <- model.cv$lambda.min
        lambda.matrix[i, 4] <- model.cv$lambda.1se
    }
    
    lambda.matrix <- log( lambda.matrix )
    
    plot ( lambda.matrix[ , 1], type = "l", col = "red", ylim = y.limit, main = "Lasso") # Min
    lines( lambda.matrix[ , 2], type = "l", col = "blue" )                               # Max
    lines( lambda.matrix[ , 3], type = "l", col = "orange" )                             # L Min
    lines( lambda.matrix[ , 4], type = "l", col = "green" )                              # L 1se
}


# Create Dummy Variable Matrix for cv.glmnet
dummy.matrix <- model.frame(split$train)
dummy.matrix <- stats::model.matrix( dummy.matrix, data = dummy.matrix )


# Identify Optimum Lambda Values
# get.lambda.lasso( dummy.matrix, c(4, 15) ) # Uncomment to Run Lambda Search

lambda.values.r <- NULL
lambda.values.l <- c( exp(5.0), exp(5.5), exp(6.0), exp(6.5), exp(7.0), exp(7.5) )

```



```{r}

# Variable Selection
# ==================================================

full.model <- lm( Sale_Price ~ . - PID, split$train )

# AIC
model.aic.f <- step( lm( Sale_Price ~ 1, split$train ), 
                     list(upper = full.model),
                     trace = 0, direction = "forward" )

model.aic.b <- step( full.model, trace = 0, direction = "backward" )
  
# BIC
model.bic.f <- step( lm( Sale_Price ~ 1, split$train ), 
                     list(upper = full.model),
                     trace = 0, direction = "forward", k = log( nrow(split$train) ) )

model.bic.b = step( full.model, trace = 0, direction = "backward", k = log( nrow(split$train) ) )

which.max( c( summary(model.aic.f)$adj.r.squared, summary(model.aic.b)$adj.r.squared,
              summary(model.bic.f)$adj.r.squared, summary(model.bic.b)$adj.r.squared) )

# Significant Variables
pred.vars       <- attr( terms(model.aic.b), "predvars" )
pred.vars.names <- vector()

for ( i in 2:length(pred.vars) )
{
  pred.vars.names[i - 1] <- as.character( pred.vars[[i]] )
}

sig.train.x <- cbind("PID" = split$train$PID, split$train[, pred.vars.names[2:length(pred.vars.names)] ] )
sig.train.y <- split$train[, pred.vars.names[1] ]

sig.test.x <- cbind("PID" = split$test$PID, split$test[, pred.vars.names[2:length(pred.vars.names)] ] )
sig.test.y <- split$test[, pred.vars.names[1] ]


# # Partial Least Squares
# pls.model <- plsr( Sale_Price ~ ., data = split$train, validation = "CV" )
# cv        <- RMSEP(pls.model)
# best.dims <- which.min( cv$val[estimate = "adjCV", , ] ) - 1
#  
# # Rerun the model
# pls.model <- plsr( Sale_Price ~ ., data = split$train, ncomp = best.dims )
# 
# pred.vars       <- attr( terms(pls.model), "predvars" )
# pred.vars.names <- vector()
# 
# for ( i in 2:length(pred.vars) )
# {
#   pred.vars.names[i - 1] <- as.character( pred.vars[[i]] )
# }
# 
# sig.train.x <- cbind("PID" = split$train$PID, split$train[, pred.vars.names[2:length(pred.vars.names)] ] )
# sig.train.y <- split$train[, pred.vars.names[1] ]
# 
# sig.test.x <- cbind("PID" = split$test$PID, split$test[, pred.vars.names[2:length(pred.vars.names)] ] )
# sig.test.y <- split$test[, pred.vars.names[1] ]

```



```{r}

# Generate Dummy Matrices
# ==================================================

dummy.train.x <- model.frame( cbind( rep(1, nrow(dummy.train.x) ), sig.train.x ) )
dummy.train.x <- stats::model.matrix( dummy.train.x, data = dummy.train.x )

dummy.test.x <- model.frame( cbind( rep(1, nrow(dummy.test.x) ), sig.test.x ) )
dummy.test.x <- stats::model.matrix( dummy.test.x, data = dummy.test.x )

```



```{r}

# Model 1 - GLMNET
# ==================================================

# Fit GLMNET
model.cv <- cv.glmnet( as.matrix( dummy.train.x[, -c(1:2)] ), 
                       as.matrix( sig.train.y ), 
                       alpha = 1, lambda = NULL )
                          
best.lambda <- model.cv$lambda.min
cv.pred     <- round( predict( model.cv, s = best.lambda, newx = dummy.test.x[, -c(1:2)] ), 2 )

# Get MSPE
sqrt( mean( ( log(cv.pred) - log(sig.test.y) ) ^ 2 ) )

```



```{r}

# Model 2 - XGBoost
# ==================================================

xg.train <- xgb.DMatrix(data = dummy.train.x[, -c(1:2)], label = sig.train.y) 
xg.test  <- xgb.DMatrix(data = dummy.test.x [, -c(1:2)], label = sig.test.y) 

params <- list(booster          = "gbtree", 
               objective        = "reg:linear", 
               eta              = 0.35, 
               gamma            = 0, 
               max_depth        = 6, 
               min_child_weight = 1, 
               subsample        = 1, 
               colsample_bytree = 1)

xg.cv <- xgb.cv(params           = params, 
                data             = xg.train, 
                nrounds          = 100, 
                nfold            = 10, 
                showsd           = T, 
                stratified       = T,
                early_stop_round = 20, 
                maximize         = F,
                print_every_n    = 10)

best.iter <- which( xg.cv[[4]][, "test_rmse_mean" ] == min( xg.cv[[4]][, "test_rmse_mean" ] ) )


xg.train.1 <- xgb.train( params           = params, 
                         data             = xg.train, 
                         nrounds          = best.iter, 
                         watchlist        = list(val = xg.test, train = xg.train), 
                         early_stop_round = 10, 
                         maximize         = F , 
                         eval_metric      = "rmse",
                         print_every_n    = 10 )

xg.predict <- round( predict( xg.train.1, xg.test ), 2 )

sqrt( mean( ( log(xg.predict) - log(sig.test.y) ) ^ 2 ) )

```



```{r, warning = FALSE, echo = FALSE, message = FALSE}

# Save Output Function
# ==================================================

save.output <- function(id, predictions, number)
{
  file.data <- cbind( "PID" = id, "Sale_Price" = round(predictions, 2) )
    
  write.table( file.data, file = paste( "mysubmission", number, ".txt", sep = "" ), sep = ", ",
              row.names = FALSE, col.names = c( "PID", "Sale_Price" )  )
}

save.output( dummy.test.x[, "PID"], cv.pred,    1 )
save.output( dummy.test.x[, "PID"], xg.predict, 2 )

```

```{r, warning = FALSE, echo = FALSE, message = FALSE}

# Evaluation Function
# ==================================================

evaluate <- function(test.y) 
{
  pred             <- read.csv("mysubmission1.txt")
  names(test.y)[2] <- "True_Sale_Price"
  pred             <- merge(pred, test.y, by = "PID")
  
  sqrt( mean( ( log(pred$Sale_Price) - log(pred$True_Sale_Price) ) ^ 2 ) )
}

```
