---
title:  "Project 1 4042 sbishop3"
author: "Scott Bishop"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

- A report - 3 pages maximum, .pdf only, that provides the details of your code, e.g., pre-processing, some technical details or implementation details (if not trivial) of the models you use, etc.

- In addition, report the accuracy (see evaluation metric given below), running time of your code and the computer system you use (e.g., Macbook Pro, 2.53 GHz, 4GB memory, or AWS t2.large). You DO NOT need to submit the part of the code related to the evaluation you conduct.

- Build TWO prediction models. Always include a tree-based ensemble model, e.g., randomForest, and/or boosting tree. 

- After running your Rcode, we should see TWO txt files in the same directory named "mysubmission1.txt" and "mysubmission2.txt". Each submission file correspoonds to a prediction on the test data.

- For each of your two models, prediction error is calculated as the averaged RMSE over 3 splits. Full credit if one of the two errors is below 0.132 and extra credit if below 0.120

```{r, warning = FALSE, echo = FALSE, message = FALSE, include = FALSE}

# Set up Environment
# ==================================================

ames <- read.csv("./data/ames.csv")
set.seed(4042)

```

```{r, warning = FALSE, echo = FALSE, message = FALSE, include = FALSE}

# Install Packages
# ==================================================
packages <- c("DescTools", "tree")   
init.pkg <- setdiff( packages, rownames( installed.packages() ) )  

if ( length(init.pkg) > 0 ) 
{
    install.packages(init.pkg)
} 

lapply(packages, require, character.only = TRUE)

```




```{r, warning = FALSE, echo = FALSE, message = FALSE}

# Partition Function
# ==================================================

make.split <- function(data)
{
    indexes <- sample( seq_len(nrow(data)), 
                       size = round(nrow(data) * 0.70) )

    train.csv  <- data[ indexes, ]
    test.csv   <- data[-indexes, ]
    test.csv.y <- test.csv[,  ncol(test.csv)]
    test.csv.x <- test.csv[, -ncol(test.csv)]
    
    return( list("train"  = train.csv,
                 "test.x" = test.csv.x,
                 "test.y" = test.csv.y) )
}

split <- make.split(ames)

```

```{r, warning = FALSE, echo = FALSE, message = FALSE}

# Evaluation Function
# ==================================================

evaluate <- function(test.y) 
{
  pred             <- read.csv("mysubmission1.txt")
  names(test.y)[2] <- "True_Sale_Price"
  pred             <- merge(pred, test.y, by = "PID")
  
  sqrt( mean( ( log(pred$Sale_Price) - log(pred$True_Sale_Price) ) ^ 2 ) )
}

```

```{r, warning = FALSE, echo = FALSE, message = FALSE}

# Save Output Function
# ==================================================

save.output <- function(predictions, number)
{
  write.table(predictions, file = paste( "mysubmission", number, ".txt", sep = "" ), sep = ", ",
              row.names = FALSE, col.names = c( "PID", "Sale_Price" )  )
}

```



TIPS

Winsorization. Specifically, substitute 95% (or 5%) percentile for the data above 95% (or below 5%) percentile. Winsorization is a transformation technique to limit extreme values and reduce the effect of potential outliers. 

Fit a linear model with Lasso regularization. Tune by cross validation and use lambda.min.

The two methods can be the same algorithm with two sets of parameters (i.e., XGboost with two sets of tuning parameters)


```{r}

# Preprocessing
# ==================================================

# Find Majority Factor Function
get.maj.factors <- function(vars, thresh)
{
  vars.to.remove <- character()
  denominator    <- nrow(split$train)
  
  # For Each Factor Variable
  for ( var in 1:length(vars) )
  {
    # And For Each Level of Current Factor Variable
    for ( level in 1:length( table(vars[[var]]) ) )
    {
      # Calculate it's Proportion of Rows
      numerator  <- table( vars[[var]] )[[level]]
      maj.factor <- numerator / denominator
      
      # If this Level has 95% Majority, Flag it For Removal
      if ( maj.factor >= thresh )
      {
        vars.to.remove[ length(vars.to.remove) + 1 ] <- names(factor.vars[var])
        level <- length( table(vars[[var]]) )
      }
    }
  }
  
  return(vars.to.remove)
}


# -------------------------

# Remove Longitude and Latitude
split$train <- split$train[, -c(81, 82)]

# Identify & Remove Majority Factors Variables
factor.vars  <- split$train[, sapply(split$train, is.factor) ]
drop.columns <- get.maj.factors(factor.vars, 0.95)
split$train  <- split$train[, !( names(split$train) %in% drop.columns ) ]

```

```{r}

# Refactorize Factor Variables
# ==================================================

# Overall_QUal
levels(split$train$Overall_Qual)[levels(split$train$Overall_Qual) %in% c("Very_Poor", "Poor", "Fair",  "Below_Average" ) ]  <- "low"
levels(split$train$Overall_Qual)[levels(split$train$Overall_Qual) %in% c("Average",   "Above_Average", "Good" ) ]           <- "avg"
levels(split$train$Overall_Qual)[levels(split$train$Overall_Qual) %in% c("Very_Good", "Excellent",     "Very_Excellent" ) ] <- "high"

# -------------------------

# Year Partition Function
get.year.partitions <- function(x)
{
  # Build Tree
  tree.model <- tree(Sale_Price ~ x, data = split$train, mindev = 0.0015)
  left.split <- tree.model$frame$splits[, 1]
  splits     <- left.split[left.split != ""]
  
  # Coerce Leaf Nodes into Integers
  for ( leaf in 1:length(splits) )
  {
    splits[leaf] <- gsub( "<", "", splits[leaf] )
  }
  
  splits <- as.numeric(splits)
}


# Set New Year Levels
set.year.levels <- function(nodes, variable)
{
  variable     <- as.numeric( variable )
  new.variable <- rep( 0, length(variable) )
  
  min.year <- floor( min(nodes) )
  max.year <- floor( max(nodes) )
  
  # For All Rows Above Min and Below Max
  for ( row in 1:length(variable) )
  {
    # Assign Lower and Upper Bound Nodes
    if ( variable[row] <= min.year ) { new.variable[row] <- paste("<", min.year, sep = "" ) }
    if ( variable[row] >= max.year ) { new.variable[row] <- paste(">", max.year, sep = "" ) }
    
    # For Each Possible Category
    for ( node in 2:( length(nodes) ) ) {
      # Check If Node Falls into Interval
      if ( variable[row] <= nodes[node] && new.variable[row] == 0 )
      {
        left              <- as.character( floor( nodes[node - 1] ) )
        right             <- as.character( floor( nodes[node    ] ) )
        new.variable[row] <- paste( left, " - ", right, sep = "" )
      }
    }  
  }
  
  return(new.variable)
}

# -------------------------

# New Year_Built Levels
year_built.levels      <- sort( get.year.partitions(split$train$Year_Built) )
split$train$Year_Built <- factor( set.year.levels(year_built.levels, split$train$Year_Built) )

```



```{r}

# Locate Columns with Missing Values
na.columns <- colnames( split$train )[ colSums( is.na( split$train ) ) > 0 ]

# Replace Missing Values
set.missing.values <- function(the.data)
{
  train <- the.data
  
  # For Each Variable
  for( col in 1:ncol(train) )
  {
    # If Numeric Replace with Mean
    if ( is.numeric( train[, col ] ) )
    {
      train[ is.na( train[, col ] ), col ] <- round( mean( train[, col ], na.rm = TRUE), digits = 0 )
    }
    
    # If Categorical Replace with Most Common Category
    else if ( is.character( train[, col ] ) )
    {
      train[ is.na( train[, col ] ), col ] <- 
        names( table( train[, col ] )[ which.max( table( train[, col ] ) ) ] )
    }
  }
  
  return(train) 
}

if ( length(na.columns) > 0 ) { split$train <- set.missing.values(split$train) }

```



```{r}

# Winsorization
summary(split$train)
split$train <- Winsorize( split$train, minval = NULL, maxval = NULL, probs = c(0.05, 0.95),  na.rm = FALSE )

```





```{r}

# Model 1 (Linear Regression)
# ==================================================

```

```{r}

# Model 1 Predictions Output
# ==================================================

```




```{r}

# Model 2
# ==================================================

```

```{r}

# Model 1 Predictions Output
# ==================================================

```
