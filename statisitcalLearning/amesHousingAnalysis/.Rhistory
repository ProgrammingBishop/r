b2 <- 0.07
b3 <- 35
b4 <- 0.01
b5 <- -10
b0 <- b0
b1 <- b1 + (b4 * x2) + (b5 * x3)
b0
b1
b2
b3 * x3
b3 <- (b3 * x3)
b3
b4 <- (b4 * x2) + (b5 * x3)
b4
# ============================================================
# Predictor value
x1 <- 4.0
x2 <- 100
# Average effect on Y for unit increase in X
b0 <- 50
b1 <- 20
b2 <- 0.07
b3 <- 35
b4 <- 0.01
b5 <- -10
x3 <- 1
b0 <- b0
b1 <- b1 + (b4 * x2) + (b5 * x3)
b3 <- (b3 * x3)
b4 <- (b4 * x2) + (b5 * x3)
b0
b1
b3
b4
# Predictor value
GPA <- 4.0
IQ  <- 100
SEX <- 1
# Average effect on Y for unit increase in X
b0 <- 50
b1 <- 20
b2 <- 0.07
b3 <- 35
b4 <- 0.01
b5 <- -10
# =======================
w0 <-  2.939
w1 <-  0.046
w2 <-  0.189
w3 <- -0.001
(b2 + (b4 * x1))
b3 + (b5 * x1)
(b2 + (b4 * x1)) * x2
(b3 + (b5 * x1)) * x3
# Predictor value
GPA <- 4.0
IQ  <- 100
SEX <- 1
# Average effect on Y for unit increase in X
b0 <- 50
b1 <- 20
b2 <- 0.07
b3 <- 35
b4 <- 0.01
b5 <- -10
# Diff Interc       # Diff Slope                # Base Slope
female <- (b0 + (b3 * SEX)) + ((b1 + (b5 * SEX)) * GPA) + ((b2 + (b4 * GPA)) * IQ)
male   <-  b0 +                (b1               * GPA) + ((b2 + (b4 * GPA)) * IQ)
female
male
b0_hat <- b0
b2_hat <- b2 + (b4 * GPA)
b3_hat <- b3 * SEX
b4_hat <- b4 * GPA
b0_hat
b2_hat
b3_hat
b4_hat
# Predictor value
GPA <- 4.0
IQ  <- 100
SEX <- 1
# Average effect on Y for unit increase in X
b0 <- 50
b1 <- 20
b2 <- 0.07
b3 <- 35
b4 <- 0.01
b5 <- -10
# Diff Interc       # Diff Slope                # Base Slope
female <- (b0 + (b3 * SEX)) + ((b1 + (b5 * SEX)) * GPA) + ((b2 + (b4 * GPA)) * IQ)
male   <-  b0 +                (b1               * GPA) + ((b2 + (b4 * GPA)) * IQ)
female
male
IQ  <- 1
# Diff Interc       # Diff Slope                # Base Slope
female <- (b0 + (b3 * SEX)) + ((b1 + (b5 * SEX)) * GPA) + ((b2 + (b4 * GPA)) * IQ)
male   <-  b0 +                (b1               * GPA) + ((b2 + (b4 * GPA)) * IQ)
female
male
b0_hat <- b0
b2_hat <- b2 + (b4 * GPA)
b3_hat <- b3 * SEX
b4_hat <- b4 * GPA
b0_hat
b2_hat
b3_hat
b4_hat
19 * 1.1 * 0.289
19 * 1.1 * 0.0289
19 * 1.1 * 0.189
19 + 1.1 * 0.189
19 + 1.1 * 0.0289
b2_hat <- b2 + (b4 * GPA)
b2_hat
b3_hat <- b3 + (b5 * GPA)
b3_hat
b3_hat <- b5 + (b3 * GPA)
b3_hat
b2_hat <- b2 + (b4 * GPA)
b2_hat
b3_hat <- b3 + (b5 * GPA)
b3_hat
b4_hat <- b4 * GPA * IQ
b4_hat
# Predictor value
GPA <- 4.0
IQ  <- 100
SEX <- 1
# Average effect on Y for unit increase in X
w0 <- 50
w1 <- 20
w2 <- 0.07
w3 <- 35
w4 <- 0.01
w5 <- -10
w1 * GPA
e1 <- w1 + (w3 * IQ)
e1
e2 <- w2 + (w3 * GPA)
e2
e0 <- w0 + (w3 * GPA * IQ)
e0
e0 <- a0 + (a3 * SEX)
a0 <- 50
a1 <- 20
a2 <- 0.07
a4 <- 0.01
a3 <- 35
a5 <- -10
male   <- (a0 + (a3 * SEX)) + ((a1 + (a5 * SEX)) * GPA) + ((a2 + (a4 * GPA)) * IQ)
female <-  a0 +                (a1               * GPA) + ((a2 + (a4 * GPA)) * IQ)
e0 <- a0 + (a3 * SEX)
e0
b2 * IQ
IQ  <- 1
b2 * IQ
b2_hat <- b2 + (b4 * GPA)
b2_hat
1 * 0.07
100 / 0.07
0.07 * 1428.571
# Predictor value
GPA <- 4.0
IQ  <- 1
SEX <- 1
# Average effect on Y for unit increase in X
b0 <- 50
b1 <- 20
b2 <- 0.07
b3 <- 35
b4 <- 0.01
b5 <- -10
b2_hat <- b2 + (b4 * GPA) * IQ
b2_hat
b2 + (b4 * GPA)
b2_hat <- b2 + (b4 * GPA) * IQ
b2_hat
IQ  <- 100
b2_hat <- b2 + (b4 * GPA) * IQ
b2_hat
b4_hat <- b4 * GPA * IQ
b4_hat
# Predictor value
GPA <- 4.0
IQ  <- 100
SEX <- 1
# Average effect on Y for unit increase in X
w0 <- 50
w1 <- 20
w2 <- 0.07
w3 <- 35
w4 <- 0.01
w5 <- -10
e0 <- w0 + (w3 * GPA * IQ)
e0
IQ  <- 1
female <- w0 + (w1 + (w3 * IQ)) * GPA) + (w2 * IQ)
female <- w0 + ((w1 + (w3 * IQ)) * GPA) + (w2 * IQ)
female
# Predictor value
GPA <- 4.0
IQ  <- 100
SEX <- 1
# Average effect on Y for unit increase in X
w0 <- 50
w1 <- 20
w2 <- 0.07
w3 <- 35
w4 <- 0.01
w5 <- -10
w1 + (w3 * IQ)
(w1 + (w3 * IQ)) * GPA
w1 * GPA
w2 * IQ
e0 <- w0 + ((w1 + (w3 * IQ)) * GPA)
e0
e1 <- (w1 + (w3 * IQ)) * GPA
e1
e2 <- (w2 + (w3 * GPA)) * IQ
e2
e0 <- w0 + (w3 * GPA * IQ)
e0
(w1 + (w3 * IQ))
(w1 + (w3 * IQ)) * GPA
(w2 + (w3 * GPA)) * IQ
(w3 * GPA * IQ)
(w2 + (w3 * GPA)) * IQ
(w2 + (w3 * GPA))
GPA <- 4.0
IQ  <- 100
SEX <- 1
a0 <- 50
a1 <- 20
a2 <- 0.07
a3 <- 35
a4 <- 0.01
a5 <- -10
a2 * IQ
a1 * GPA
((a2 + (a4 * GPA)) * IQ)
(a2 + (a4 * GPA))
(a2 + (a4 * IQ))
a4 * GPA * IQ
a1 * GPA
a1 <- 20
a1 * GPA
b2 * IQ
(b2 * IQ) + (b4 * GPA * IQ)
a2 * IQ
(a2 + (a4 * GPA)) * IQ
(a2 + (a4 * GPA))
4 * 0.07
(a1 + (a4 * IQ) + (a5 * SEX)) * GPA
(a2 + (a4 * GPA)) * IQ
(a1 + (a5 * SEX)) * GPA
(a3 + (a5 * GPA)) * SEX
a1 * GPA
(a1 + (a5 * SEX)) * GPA
a2 * IQ
(a1 + (a4 * IQ) + (a5 * SEX)) * GPA
(a3 + (a5 * GPA)) * SEX
a5 * GPA * SEX
67.74 * 0.04 / 100
data(BostonHousing1)
library(MASS)
attach(Boston)
install.packages("splines")
library(splines)
model <- bs(nox ~ poly(dis, 3), data = Boston)
head(Boston)
?bs
model <- lm(nox ~ poly(dis, 3), data = Boston)
summary(model)
sum( (Boston$nox - model$residuals)^2 )
sum(resid(model)^2)
predict(model, newdata = data.frame("dis" = 6))
head(Boston)
summary(model)$coefficitents
summary(model)$coefficients
sumary(model)
summary(model)
4.27e-07
model <- lm(nox ~ poly(dis, 4), data = Boston)
sum(resid(model)^2)
predict(model, newdata = data.frame("dis" = 6))
summary(model)
myfit1 = lm(nox ~ bs(dis, df=3), data=Boston)
dim(myfit1)
dim(bs(dis, df=3))
dim(dis, knots=median(dis))
dim(bs(dis, knots=median(dis)))
dim(poly(dis, 3)
)
dim(bs(dis, df = 5, intercept = TRUE))
dim(bs(dis, knots = quantile(dis, prob = c(0.25, 0.5, 0.75))))
dim(bs(dis, df = 4, intercept = TRUE))
myfit1 = lm(nox ~ bs(dis, df=3), data=Boston)
myfit2 = lm(nox ~ (bs(dis, knots=median(dis)), data=Boston))
predict(myfit1, newdata = data.frame("dis" = 6))
myfit2 = lm(nox ~ bs(dis, knots=median(dis)), data=Boston)
predict(myfit2, newdata = data.frame("dis" = 6))
myfit1 = lm(nox ~ bs(dis, df=3), data=Boston)
myfit2 = lm(nox ~ bs(dis, knots=median(dis)), data=Boston)
myfit3 = lm(nox ~ poly(dis, 3), data=Boston)
myfit4 = lm(nox ~ bs(dis, df = 4, intercept = TRUE), data=Boston)
predict(myfit1, newdata = data.frame("dis" = 6))
predict(myfit2, newdata = data.frame("dis" = 6))
predict(myfit3, newdata = data.frame("dis" = 6))
predict(myfit4, newdata = data.frame("dis" = 6))
myfit4 = lm(nox ~ bs(dis, df = 4, intercept = TRUE), data=Boston)
predict(myfit4, newdata = data.frame("dis" = 6))
myfit5 = lm(nox ~ bs(dis, df = 5, intercept = TRUE), data=Boston)
myfit6 = lm(nox ~ bs(dis, knots = quantile(dis, prob = c(0.25, 0.5, 0.75))), data=Boston)
predict(myfit5, newdata = data.frame("dis" = 6))
predict(myfit6, newdata = data.frame("dis" = 6))
myfit2 = lm(nox ~ bs(dis, df=4), data=Boston)
myfit2 = lm(nox ~ bs(dis, df=4), data=Boston)
a <- lm(nox ~ ploy(dis, 3), data=Boston)
a <- lm(nox ~ poly(dis, 3), data=Boston)
b <- lm(nox ~ bs(dis, df = 5, intercept = TRUE), data=Boston)
c <- lm(nox ~ bs(dis, knots = quantile(dis, prob = c(0.25, 0.5, 0.75))), data=Boston)
d <- lm(nox ~ bs(dis, df = 4, intercept = TRUE), data=Boston)
e <- lm(nox ~ bs(dis, knots = median(dis)), data=Boston)
predict(a, newdata = data.frame("dis" = 6))
predict(b, newdata = data.frame("dis" = 6))
myfit2 = lm(nox ~ bs(dis, df=4), data=Boston)
a <- lm(nox ~ poly(dis, 3), data=Boston)
b <- lm(nox ~ bs(dis, df = 5, intercept = TRUE), data=Boston)
c <- lm(nox ~ bs(dis, knots = quantile(dis, prob = c(0.25, 0.5, 0.75))), data=Boston)
d <- lm(nox ~ bs(dis, df = 4, intercept = TRUE), data=Boston)
e <- lm(nox ~ bs(dis, knots = median(dis)), data=Boston)
predict(myfit2, newdata = data.frame("dis" = 6))
predict(a, newdata = data.frame("dis" = 6))
predict(b, newdata = data.frame("dis" = 6))
predict(c, newdata = data.frame("dis" = 6))
predict(d, newdata = data.frame("dis" = 6))
predict(e, newdata = data.frame("dis" = 6))
?smooth.spline
model <- lm(nox ~ poly(dis, 4), data = Boston)
predict(model, newdata = data.frame("dis" = 6))
ames <- read.csv("./data/ames.csv")
set.seed(4042)
getwd()
setwd('D:\\git\\rMachineLearning')
ames <- read.csv("./data/ames.csv")
set.seed(4042)
setwd('D:\\git\\rMachineLearning\\statisitcalLearning\\amesHousingAnalysis')
ames <- read.csv("./data/ames.csv")
set.seed(4042)
# Install Packages
# ==================================================
packages <- c("DescTools", "tree", "glmnet")
init.pkg <- setdiff( packages, rownames( installed.packages() ) )
if ( length(init.pkg) > 0 ) { install.packages(init.pkg) }
lapply(packages, require, character.only = TRUE)
make.split <- function(data)
{
indexes <- sample( seq_len(nrow(data)),
size = round(nrow(data) * 0.70) )
train.csv  <- data[ indexes, ]
test.csv   <- data[-indexes, ]
test.csv.y <- test.csv[,  ncol(test.csv)]
test.csv.x <- test.csv[, -ncol(test.csv)]
return( list("train"  = train.csv,
"test.x" = test.csv.x,
"test.y" = test.csv.y) )
}
split <- make.split(ames)
get.maj.factors <- function(vars, thresh)
{
vars.to.remove <- character()
denominator    <- nrow(split$train)
# For Each Factor Variable
for ( var in 1:length(vars) )
{
# And For Each Level of Current Factor Variable
for ( level in 1:length( table(vars[[var]]) ) )
{
# Calculate it's Proportion of Rows
numerator  <- table( vars[[var]] )[[level]]
maj.factor <- numerator / denominator
# If this Level has 95% Majority, Flag it For Removal
if ( maj.factor >= thresh )
{
vars.to.remove[ length(vars.to.remove) + 1 ] <- names(factor.vars[var])
level <- length( table(vars[[var]]) )
}
}
}
return(vars.to.remove)
}
# Remove Longitude and Latitude
split$train <- split$train[, -c(81, 82)]
# Identify & Remove Majority Factors Variables
factor.vars  <- split$train[, sapply(split$train, is.factor) ]
drop.columns <- get.maj.factors(factor.vars, 0.95)
split$train  <- split$train[, !( names(split$train) %in% drop.columns ) ]
# Overall_QUal
levels(split$train$Overall_Qual)[levels(split$train$Overall_Qual) %in% c("Very_Poor", "Poor", "Fair",  "Below_Average" ) ]  <- "low"
levels(split$train$Overall_Qual)[levels(split$train$Overall_Qual) %in% c("Average",   "Above_Average", "Good" ) ]           <- "avg"
levels(split$train$Overall_Qual)[levels(split$train$Overall_Qual) %in% c("Very_Good", "Excellent",     "Very_Excellent" ) ] <- "high"
# Year Partition Function
get.year.partitions <- function(x)
{
# Build Tree
tree.model <- tree(Sale_Price ~ x, data = split$train, mindev = 0.0015)
left.split <- tree.model$frame$splits[, 1]
splits     <- left.split[left.split != ""]
# Coerce Leaf Nodes into Integers
for ( leaf in 1:length(splits) )
{
splits[leaf] <- gsub( "<", "", splits[leaf] )
}
splits <- as.numeric(splits)
}
# Set New Year Levels
set.year.levels <- function(nodes, variable)
{
variable     <- as.numeric( variable )
new.variable <- rep( 0, length(variable) )
min.year <- floor( min(nodes) )
max.year <- floor( max(nodes) )
# For All Rows Above Min and Below Max
for ( row in 1:length(variable) )
{
# Assign Lower and Upper Bound Nodes
if ( variable[row] <= min.year ) { new.variable[row] <- paste("<", min.year, sep = "" ) }
if ( variable[row] >= max.year ) { new.variable[row] <- paste(">", max.year, sep = "" ) }
# For Each Possible Category
for ( node in 2:( length(nodes) ) ) {
# Check If Node Falls into Interval
if ( variable[row] <= nodes[node] && new.variable[row] == 0 )
{
left              <- as.character( floor( nodes[node - 1] ) )
right             <- as.character( floor( nodes[node    ] ) )
new.variable[row] <- paste( left, " - ", right, sep = "" )
}
}
}
return(new.variable)
}
# New Year_Built Levels
year_built.levels      <- sort( get.year.partitions(split$train$Year_Built) )
split$train$Year_Built <- factor( set.year.levels(year_built.levels, split$train$Year_Built) )
na.columns <- colnames( split$train )[ colSums( is.na( split$train ) ) > 0 ]
# Replace Missing Values
set.missing.values <- function(the.data)
{
train <- the.data
# For Each Variable
for( col in 1:ncol(train) )
{
# If Numeric Replace with Mean
if ( is.numeric( train[, col ] ) )
{
train[ is.na( train[, col ] ), col ] <- round( mean( train[, col ], na.rm = TRUE), digits = 0 )
}
# If Categorical Replace with Most Common Category
else if ( is.character( train[, col ] ) )
{
train[ is.na( train[, col ] ), col ] <-
names( table( train[, col ] )[ which.max( table( train[, col ] ) ) ] )
}
}
return(train)
}
if ( length(na.columns) > 0 ) { split$train <- set.missing.values(split$train) }
# Winsorize Numerical Values
winsorize.values <- function(data)
{
train <- data
# For Each Variable
for( col in 2:(ncol(train) - 1) )
{
# Winsorize Numerical Columns
if ( is.numeric( train[, col ] ) )
{
train[, col ] <- Winsorize( train[, col ],
minval = NULL, maxval = NULL,
probs = c(0.05, 0.95), na.rm = FALSE )
}
}
return(train)
}
split$train <- winsorize.values(split$train)
get.lambda.lasso <- function(data, y.limit)
{
lambda.matrix <- matrix( 0, nrow = 50, ncol = 4 )
for ( i in 1:nrow( lambda.matrix ) )
{
model.cv <- cv.glmnet( as.matrix( data[, 2:( ncol(data) - 1) ] ),
as.matrix( data[, ncol(data) ] ) )
lambda.matrix[i, 1] <- min(model.cv$lambda)
lambda.matrix[i, 2] <- max(model.cv$lambda)
lambda.matrix[i, 3] <- model.cv$lambda.min
lambda.matrix[i, 4] <- model.cv$lambda.1se
}
lambda.matrix <- log( lambda.matrix )
plot ( lambda.matrix[ , 1], type = "l", col = "red", ylim = y.limit, main = "Lasso") # Min
lines( lambda.matrix[ , 2], type = "l", col = "blue" )                               # Max
lines( lambda.matrix[ , 3], type = "l", col = "orange" )                             # L Min
lines( lambda.matrix[ , 4], type = "l", col = "green" )                              # L 1se
}
# Create Dummy Variable Matrix for cv.glmnet
dummy.matrix <- model.frame(split$train)
dummy.matrix <- stats::model.matrix( dummy.matrix, data = dummy.matrix )
# Identify Optimum Lambda Values
get.lambda.lasso( dummy.matrix, c(-10, 10) )
lambda.values.r <- NULL
lambda.values.l <- c( exp(5.0), exp(5.5), exp(6.0), exp(6.5), exp(7.0), exp(7.5) )
tinytex::install_tinytex()
