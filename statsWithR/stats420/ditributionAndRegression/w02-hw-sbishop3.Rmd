---
title:  "Week II"
author: "STAT 420, Summer 2018, Scott Bishop ~ sbishop3"
date:   ''
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

# Exercise 1 (Using `lm`)

```{r, eval = FALSE, warning = FALSE}

Sys.setenv(TZ = "America/Chicago")
setwd( '/Users/Sbishop/Downloads/w02-hw-sbishop3' )

```

### A 

```{r}

library(MASS)
cat.model <- lm( Hwt ~ Bwt, data = cats )
summary( cat.model )

```


### B  
##### Output only the estimated regression coefficients. Interpret $\hat{\beta_0}$ and $\beta_1$ in the *context of the problem*. Be aware that only one of those is an estimate.

```{r}

beta_hat_0 <- cat.model$coefficients[1]
beta_hat_1 <- cat.model$coefficients[2]
c( beta_hat_0, beta_hat_1 )

```

$\beta_1$ is the ideal optimal slope. That is, for a mean change in cat height we can expect change in cat weight holding other predictors in the model constant.

$\hat{\beta_0}$ is the expected value of the response at a given predictor value x.


### C  
##### Use your model to predict the heart weight of a cat that weights **2.7** kg. Do you feel confident in this prediction? Briefly explain.

```{r}

min( cats$Bwt ) < 2.7 & 2.7 < max( cats$Bwt )
Y <- beta_hat_0 + ( beta_hat_1 * 2.7 )
Y

```

I can feel confident about this prediction becuase the value falls within the data seen, therefore interpolation can be executed (not extrapolation).


### D  
##### Use your model to predict the heart weight of a cat that weights **4.4** kg. Do you feel confident in this prediction? Briefly explain.

```{r}

min( cats$Bwt ) < 4.4 & 4.4 < max( cats$Bwt )
Y <- beta_hat_0 + ( beta_hat_1 * 4.4 )
Y

```

I do not feel confident in this estimate because the predictor value falls outside of the data seen and therefore I am forced into extrapolation.


### E  
##### Create a scatterplot of the data and add the fitted regression line. Make sure your plot is well labeled and is somewhat visually appealing.

```{r}

plot( y = cats$Hwt, x = cats$Bwt,
    main = "Regressions for Cat's Heart Weight\n Predicted by Body Weight",
    xlab = "Cat's Body Weight",
    ylab = "Cat's Heart Weight",
    col  = "red" )

abline( cat.model, col = 'blue' )

```


### F  
##### Report the value of $R^2$ for the model. Do so directly. Do not simply copy and paste the value from the full output in the console after running `summary()` in part **(a)**.

```{r}

y_hat    = beta_hat_0 + ( beta_hat_1 * cats$Bwt )
SST      = sum( ( cats$Hwt - mean( cats$Hwt ) ) ^ 2 )
SSReg    = sum( ( y_hat    - mean( cats$Hwt ) ) ^ 2 )
R.square = SSReg / SST

# 0.6466
R.square 

```



# Exercise 2 (Writing Functions)

### A  
##### Write a function called `get_sd_est` that calculates an estimate of $\sigma$ in one of two ways depending on input to the function. The function should take three arguments as input:

```{r}

get_sd_est <- function( fitted_values, actual_values, mle = FALSE ) 
{
    # s_e^2 = sum( e^2 ) / ( n - 2 )
    # s_e   = sqrt( s_e^2 )
    
    e <- ( actual_values - fitted_values )
    
    if ( mle == FALSE )
    {
        return ( 
            sqrt( sum( e^2 ) / ( length(fitted_values) - 2 ) )
        )
    } else {
        return ( 
            sqrt( sum( e^2 ) / length(fitted_values) )
        ) 
    }
}

```


### B  
##### Run the function `get_sd_est` on the residuals from the model in Exercise 1, with `mle` set to `FALSE`. Explain the resulting estimate in the context of the model.

```{r}

get_sd_est( cat.model$fitted.values, cats$Hwt, mle = FALSE )

```


### C  
##### Run the function `get_sd_est` on the residuals from the model in Exercise 1, with `mle` set to `TRUE`. Explain the resulting estimate in the context of the model. Note that we are trying to estimate the same parameter as in part **(b)**.

```{r}

get_sd_est( cat.model$fitted.values, cats$Hwt, mle = TRUE )

```

This estimate of $\sigma$ tells us that the predicted value in the response variable $\hat{y}$ can range 1.452373 standard deviations from the mean of the fitted line modeled by $\beta_0 + \beta_0x$.


### D  
##### To check your work, output `summary(cat_model)$sigma`. It should match at least one of **(b)** or **(c)**.

```{r}

summary(cat.model)$sigma == get_sd_est( cat.model$fitted.values, cats$Hwt, mle = FALSE )

```



# Exercise 3 (Simulating SLR)

```{r}

birthday = 19920917
set.seed(birthday)

```

### A  

```{r}

x = runif( n = 25, 0, 10 )

sim.slr <- function( x, beta_0, beta_1, sigma )
{
    n = length(x)
    epsilon = rnorm( n, mean = 0, sd = sigma )
    y = beta_0 + ( beta_1 * x ) + epsilon
    data.frame( predictor = x, response = y )
}

slr.model <- sim.slr( x, beta_0 = 5, beta_1 = -3, sigma = 3.2 )

```

### B  
##### Fit a model to your simulated data. Report the estimated coefficients. Are they close to what you would expect? Briefly explain.

```{r}

slr.lm <- lm( slr.model$response ~ slr.model$predictor )
slr.lm$coefficients

plot( y = slr.model$response, x = slr.model$predictor,
    main = "Simulated Simple Linear Regression",
    xlab = "Predictor",
    ylab = "Response",
    col  = "red" )

abline( slr.lm, col = 'blue' )

```

The coefficients are about what I would expect. By plotting the model with the fitted line it strikes through the mean of the residuals. And running a summary of the model seems to report an $R^2$ of 0.9159, which means that approximately 91.59% of the response variable variation is explained by a linear model.


### C  
##### Plot the data you simulated in part **(a)**. Add the regression line from part **(b)** as well as the line for the true model. Hint: Keep all plotting commands in the same chunk.

```{r}

beta_0 <-  5
beta_1 <- -3

plot( y = slr.model$response, x = slr.model$predictor,
    main = "Simulated Simple Linear Regression",
    xlab = "Predictor",
    ylab = "Response",
    col  = "red" )

abline( slr.lm, col = 'blue' )
abline ( beta_0, beta_1, lwd = 3, lty = 2, col = 'orange' )

```


### D  
##### Use `R` to repeat the process of simulating `n = 25` observations from the above model $1500$ times. Each time fit a SLR model to the data and store the value of $\hat{\beta_1}$ in a variable called `beta_hat_1`. Some hints:

```{r}

beta_1_hat_iter <- vector( mode = 'numeric', 1500 )

for ( i in 1:1500 )
{
    x.iter             <- runif( n = 25, 0, 10 )
    slr.iter.model     <- sim.slr( x.iter, beta_0, beta_1, 3.2 )
    lm.iter            <- lm( slr.iter.model$response ~ slr.iter.model$predictor, data = slr.iter.model )
    beta_1_hat_iter[i] <- coef(lm.iter)[2]
}

```


### E  
##### Report the mean and standard deviation of `beta_hat_1`. Do either of these look familiar?

```{r}

mean( beta_1_hat_iter )
sd( beta_1_hat_iter )

```


### F  
##### Plot a histogram of `beta_hat_1`. Comment on the shape of this histogram.

```{r}

hist( beta_1_hat_iter,
      col = 'orange',
      main = 'Slope of Multiple Simulations',
      xlab = 'Value',
      ylab = 'Frequency',
      xlim = c( -4, -2 ),
      ylim = c( 0, 300 ) )

```

The histogram in normally distributed at mean = -3.



# Exercise 4 (Be a Skeptic)

```{r}

birthday = 19920917
set.seed(birthday)

```

### A  
##### Use `R` to repeat the process of simulating `n = 75` observations from the above model $2500$ times. For the remainder of this exercise, use the following "known" values of $x$.

```{r}

beta_0 <- 3
beta_1 <- 0
slr.sd <- 2

beta_1_hat_iter <- vector( mode = 'numeric', 2500 )

for ( i in 1:2500 )
{
    x.iter             <- runif(n = 75, 0, 10)
    slr.iter.model     <- sim.slr( x.iter, beta_0, beta_1, slr.sd )
    lm.iter            <- lm( slr.iter.model$response ~ slr.iter.model$predictor, data = slr.iter.model )
    beta_1_hat_iter[i] <- coef(lm.iter)[2]
}

```

### B  
##### Plot a histogram of `beta_hat_1`. Comment on the shape of this histogram.

```{r}

hist( beta_1_hat_iter,
      col = 'orange',
      main = 'Slope of Multiple Simulations',
      xlab = 'Value',
      ylab = 'Frequency',
      xlim = c( -.35, .35 ) )

```

The histogram in normally distributed at mean = 0.


### C  
##### Import the data in [`skeptic.csv`](skeptic.csv) and fit a SLR model. The variable names in `skeptic.csv` follow the same convention as those returned by `sim_slr()`. Extract the fitted coefficient for $\beta_1$.

```{r}

df    <- as.data.frame( read.csv('./skeptic.csv') )
df.lm <- lm( response ~ predictor, data = df )
slope <- df.lm$coefficients[2]

```


### D  
##### Re-plot the histogram from **(b)**. Now add a vertical red line at the value of $\hat{\beta_1}$ in part **(c)**. To do so, you'll need to use `abline(v = c, col = "red")` where `c` is your value.

```{r}

hist( beta_1_hat_iter,
      col = 'orange',
      main = 'Slope of Multiple Simulations',
      xlab = 'Value',
      ylab = 'Frequency',
      xlim = c( -.35, .35 ) )

abline( v = slope, col = "red" )

```


### E  
##### Your value of $\hat{\beta_1}$ in **(c)** should be negative. What proportion of the `beta_hat_1` values is smaller than your $\hat{\beta_1}$? Return this proportion, as well as this proportion multiplied by `2`.

```{r}

prop <- length( beta_1_hat_iter[ beta_1_hat_iter < slope ] ) / length(beta_1_hat_iter)
prop
prop * 2

```


### F  
##### Based on your histogram and part **(e)**, do you think the [`skeptic.csv`](skeptic.csv) data could have been generated by the model given above? Briefly explain.



# Exercise 5 (Comparing Models)

For this exercise we will use the `Ozone` dataset from the `mlbench` package. You should use `?Ozone` to learn about the background of this dataset. You may need to install the `mlbench` package. If you do so, do not include code to install the package in your `R` Markdown document.

```{r}

# install.packages( 'mlbench' )
library( mlbench )
data( Ozone )
head(Ozone)
?Ozone()

```

For simplicity, we will perform some data cleaning before proceeding.

```{r}

Ozone = Ozone[, c(4, 6, 7, 8)]
colnames(Ozone) = c("ozone", "wind", "humidity", "temp")
Ozone = Ozone[complete.cases(Ozone), ]
head(Ozone)

```

### A 

```{r}

# install.packages('knitr')
library(knitr)
?kable()

```

```{r}

# Fit SLR Models
wind     <- lm( Ozone$ozone ~ Ozone$wind,     data = Ozone )
humidity <- lm( Ozone$ozone ~ Ozone$humidity, data = Ozone )
temp     <- lm( Ozone$ozone ~ Ozone$temp,     data = Ozone )

# R Square
get.r.square <- function( beta_0, beta_1, x, y ) 
{
    y_hat    = beta_0 + ( beta_1 * x )
    SST      = sum( ( y  - mean( y ) ) ^ 2 )
    SSReg    = sum( ( y_hat - mean( y ) ) ^ 2 )
    R.square = SSReg / SST 
    return ( R.square )
}

r.square.wind     <- get.r.square( wind$coefficients[1],     wind$coefficients[2],     Ozone$wind ,Ozone$ozone )
r.square.humidity <- get.r.square( humidity$coefficients[1], humidity$coefficients[2], Ozone$humidity ,Ozone$ozone )
r.square.temp     <- get.r.square( temp$coefficients[1],     temp$coefficients[2],     Ozone$temp ,Ozone$ozone )

# RMSE
get.rmse <- function( actual, fitted )
{
    sqrt(
        sum( (actual - fitted) ^ 2 ) / length(fitted)
    )
}

RMSE.wind     <- get.rmse( Ozone$ozone, wind$fitted.values )
RMSE.humidity <- get.rmse( Ozone$ozone, humidity$fitted.values )
RMSE.temp     <- get.rmse( Ozone$ozone, temp$fitted.values )

# Build Table
df <- data.frame( 'R Square' = c( r.square.wind, r.square.humidity, r.square.temp ),
                  'RMSE'     = c( RMSE.wind,     RMSE.humidity,     RMSE.temp ),
                  row.names  = c( 'wind',        'humidity',        'temp' ) )

kable( df, digits = 10, align = 'r',
       col.names = c( 'R Square', 'RMSE' ) )

```


### B  
##### Based on the results, which of the three predictors used is most helpful for predicting ozone readings? Briefly explain.

Temperature would be the most reliable for predicting ozone readings because it consists of the highest $R^2$, which means that approximately 60.42% of the response variable variation is explained by a linear model when using temp as the predictor. Also, temp has the lowest Root Mean-Square Error.



# Exercise 6 (SLR without Intercept)

### A  

```{r}

get_beta_no_int <- function( x, y )
{
    Sxy        <- sum( ( x - mean(x) ) * ( y - mean(y) ) )
    Sxx        <- sum( ( x - mean(x) ) ^ 2 )
    beta_hat_1 <- Sxy / Sxx
    return( beta_hat_1 )
}

```


### B  
##### Write your derivation in your `.Rmd` file using TeX.

\[
S_{xy} = \sum_{i = 1}^n( (x - \mu(x) ) * ( y - \mu(y) ) )
\]

\[
S_{xx} = \sum_{i = 1}^n(x - \mu(x))^2
\]

\[
\hat{\beta} = \dfrac{S_{xy}}{S_{xx}}
\]


### C  
##### Test your function on the `cats` data using body weight as `x` and heart weight as `y`. What is the estimate for $\beta$ for this data?

```{r}

round( get_beta_no_int( cats$Bwt, cats$Hwt ), 4 )

```


### D  
##### Check your work in `R`. The following syntax can be used to fit a model without an intercept:

```{r}

cats.lm    <- lm( cats$Hwt ~ cats$Bwt, data = cats )
cats.slope <- round( cats.lm$coefficients[2], 4 )
no.inter   <- round( get_beta_no_int( cats$Bwt, cats$Hwt ), 4 )

cats.slope == no.inter

```